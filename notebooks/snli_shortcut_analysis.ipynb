{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " # SNLI Shortcut analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pandas as pd, os, matplotlib.pyplot as plt, sklearn\n",
    "from sklearn import tree\n",
    "from pyirr import kappa2\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "from utils.s3_utils import download_file as download_file_s3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = \"temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_file_test_predictions = \"s3://aegovan-data/model_predictions/snli-evaluate-20240322215932/snli_1.0_test_predictions.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_file_train = \"s3://aegovan-data/nli/snli/snli_1.0_train.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_file = download_file_s3(s3_file_test_predictions, temp_dir)\n",
    "train_file = download_file_s3(s3_file_train, temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_json(json_file):\n",
    "    df = pd.read_json(json_file, orient=\"records\")\n",
    "    return df\n",
    "\n",
    "def load_jsonl(jsonl_file):\n",
    "    records = []\n",
    "    with open(jsonl_file) as f:\n",
    "        for l in f:\n",
    "            records.append(json.loads(l))\n",
    "        \n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "def add_x_y(df, x_column, y_column):\n",
    "    df[\"x\"] = df[x_column]\n",
    "    df[\"y\"] = df[y_column]\n",
    "    return df\n",
    "\n",
    "def rename_columns(df):\n",
    "    cols = {\n",
    "        \"sentence1\" : \"premise\",\n",
    "        \"sentence2\" : \"hypothesis\",\n",
    "        \"gold_label\" : \"label\"\n",
    "     }\n",
    "    df = df.rename(columns =cols )\n",
    "    return df\n",
    "def filter_empty_label(df):\n",
    "    \n",
    "    df = df[~df[\"label\"].isin([\"-\"])].copy()\n",
    "    return df\n",
    "\n",
    "def add_join_hyp_premise(df):\n",
    "    df[\"prem_hyp\"] = df.apply(lambda x: \"{}. {}\".format(x[\"premise\"], x[\"hypothesis\"]), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_nb_nli_classifier import ModelNBNliClassifier\n",
    "from model_nb_tree_nli_classifier import ModelNBTreeNliClassifier\n",
    "\n",
    "\n",
    "NAIVE_BAYES=\"Naive Bayes\"\n",
    "NAIVE_BAYES_WITH_TREE = \"Naive Bayes + Tree\"\n",
    "\n",
    "\n",
    "def add_x_json_column(df):\n",
    "    df[\"x_json\"] = df.apply(lambda r: {\"premise\" : r[\"premise\"] ,\n",
    "                                       \"hypothesis\" : r[\"hypothesis\"] ,\n",
    "                                      },\n",
    "                            axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def  train_and_predict_all_classifiers(df, min_df=None, ngram_range=(1,1), stop_words = 'english', classifiers={NAIVE_BAYES, NAIVE_BAYES_WITH_TREE}, max_words_per_class = 200, classwise_vocab=True):\n",
    "    \n",
    "    max_tree_depth = 5\n",
    "    \n",
    "    df = df.copy().pipe(add_x_json_column)\n",
    "    \n",
    "   \n",
    "    \n",
    "    classifiers_map = {\n",
    "        NAIVE_BAYES : {\"model\" : ModelNBNliClassifier( min_df=min_df, \n",
    "                                            max_words_per_class=max_words_per_class, \n",
    "                                            stop_words=stop_words, \n",
    "                                            ngram_range=ngram_range, \n",
    "                                            classwise_vocab=classwise_vocab),\n",
    "                       \"x\" : \"x\"\n",
    "                      },\n",
    "        \n",
    "        NAIVE_BAYES_WITH_TREE: { \"model\" : ModelNBTreeNliClassifier(min_df=min_df, \n",
    "                                                                    max_words_per_class=max_words_per_class, \n",
    "                                                                    stop_words=stop_words, \n",
    "                                                                    ngram_range=ngram_range, \n",
    "                                                                    classwise_vocab=classwise_vocab),\n",
    "                                \n",
    "                                \"x\" : \"x_json\"\n",
    "                                \n",
    "                               }\n",
    "    }\n",
    "   \n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    for k in filter(lambda x: x in classifiers,  classifiers_map):\n",
    "    \n",
    "        m = classifiers_map[k][\"model\"]\n",
    "        x_col = classifiers_map[k][\"x\"]\n",
    "        \n",
    "        print(f\"Training {k}\")\n",
    "        \n",
    "        m.train(df[x_col], df[\"y\"])\n",
    "        p, p_conf = m.predict(df[x_col])\n",
    "        result[k] = {\n",
    "            \"m\" : m,\n",
    "            f\"predictions\" : p,\n",
    "            f\"predictions_conf\": p_conf\n",
    "        }\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def  predict_all_classifiers(df, models_dict):\n",
    "    max_words_per_class = 10\n",
    "    max_tree_depth = 4\n",
    "    \n",
    "   \n",
    "    \n",
    "    result = {}\n",
    "    for k, v in models_dict.items():\n",
    "        m = v[\"m\"]\n",
    "        \n",
    "        p, p_conf = m.predict(df[\"x\"])\n",
    "        result[k] = {\n",
    "            \"m\" : m,\n",
    "            \"predictions\" : p,\n",
    "            \"predictions_conf\": p_conf\n",
    "        }\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isint(a):\n",
    "    try:\n",
    "        t = int(a)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def float_percent_format(x):\n",
    "     return \"{:.1f}\".format(x*100)\n",
    "\n",
    "    \n",
    "def float_format(x):\n",
    "     return \"{:.2f}\".format(x)\n",
    "\n",
    "def int_format(x):\n",
    "     return str(int(x))\n",
    "    \n",
    "def compute_kappa(r1, r2):\n",
    "    python_kappa = sklearn.metrics.cohen_kappa_score(r1,r2)\n",
    "    print(python_kappa)\n",
    "    data = {'r1': r1,\n",
    "        'r2': r2}\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    r_kappa = kappa2(df, weight=\"equal\")\n",
    "    print(r_kappa)\n",
    "    \n",
    "    result = {\n",
    "        \"python_kappa\" : python_kappa,\n",
    "        \"r_kappa\" : r_kappa.value,\n",
    "        \"r_pvalue\":r_kappa.pvalue\n",
    "        \n",
    "    }\n",
    "    return result\n",
    "    \n",
    "     \n",
    "\n",
    "def print_report(actual, pred, labels=None, label_names=[]):\n",
    "      \n",
    "    d = sklearn.metrics.classification_report(actual,\n",
    "                                            pred,\n",
    "                                            output_dict=True,\n",
    "                                            zero_division=0)\n",
    "    # Compute K\n",
    "    k_scores = compute_kappa(actual, pred )\n",
    "    for k, v in k_scores.items():\n",
    "        d[k] = {\n",
    "            \"cohen\" : v,\n",
    "            \"support\" : len(actual),\n",
    "\n",
    "        }\n",
    "    df =  pd.DataFrame(d).T\n",
    "    df.insert(1, \"index\", df.index)\n",
    "\n",
    "\n",
    "    return df\n",
    "    \n",
    "def print_report_all_classifiers(actual,  results_dict, *args, **kwargs):\n",
    "    result = []\n",
    "    for k, v in results_dict.items():\n",
    "        df = print_report(actual,v[\"predictions\"],  *args, **kwargs)\n",
    "        df.insert (0, \"model\", k)\n",
    "        \n",
    "        result.append(df)\n",
    "    df = pd.concat(result)\n",
    "    print(df.to_latex(index=False, formatters = {\"precision\": float_percent_format, \"recall\":float_percent_format, \"f1-score\":float_percent_format,\"support\":int_format }))\n",
    "    return df\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtreeviz.trees import *\n",
    "\n",
    "def plot_tree(m):\n",
    "    _, ax = plt.subplots(1,1, figsize=(45,15))\n",
    "\n",
    "    tree.plot_tree(m.tree_model, ax=ax, fontsize=10, feature_names=m.feature_names, filled=True, rounded=True)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def plot_tree_pretty(m, x, y,  title=\"\", image_save_path=None):\n",
    "    import sys\n",
    "    dot_path = '/opt/homebrew/bin/'\n",
    "    if dot_path not in sys.path :\n",
    "        sys.path.append( dot_path)\n",
    "        for p in sys.path:\n",
    "            os.environ[\"PATH\"] = os.environ.get(\"PATH\",\"\") + \":\" +  p\n",
    "\n",
    "    viz = dtreeviz(m.tree_model,\n",
    "               x_data=m.extract_features(m.preprocess(x)),\n",
    "               y_data=y,\n",
    "              # target_name=target_name,\n",
    "               feature_names=m.feature_names,\n",
    "               class_names=label_names,\n",
    "               title=title)\n",
    "    \n",
    "    # tree_plt.view()    \n",
    "    if image_save_path:\n",
    "        viz.save(image_save_path)\n",
    "        \n",
    "    return viz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_discrimintary_keywords(model, label_index ) :\n",
    "    index_to_vocab = {i:v  for v,i in model.vocab.items()}\n",
    "    \n",
    "    #. ( [ (index_to_vocab[i],p) for i,p in enumerate(model.nb_model.feature_log_prob_[label_index])]\n",
    "\n",
    "    return sorted( [ (index_to_vocab[i],p) for i,p in enumerate(model.nb_model.feature_log_prob_[label_index])], \n",
    "           reverse=True, key=lambda x: x[1])[:50]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def consolidate_label_names(actual, pred, label_names):\n",
    "    label_indices = list(set(actual).union(pred))\n",
    "    return [label_names[i] for i in sorted(label_indices)]\n",
    "\n",
    "def plot_confusionmatrix(y_true,y_pred):\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    labels = consolidate_label_names( y_true,y_pred,label_names)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = labels)\n",
    "    disp.plot(cmap=\"PuBu\", xticks_rotation=\"vertical\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis(df, x_column,y_column, meta_dataset, meta_target):\n",
    "    df = df.copy(deep=True).pipe(add_x_y, x_column=\"premise\", y_column=\"label\")\n",
    "    result = train_and_predict_all_classifiers(df, min_df=1)\n",
    "    \n",
    "    all_labels = list(df[\"label\"].unique())\n",
    "    \n",
    "    df_report = print_report_all_classifiers(df[y_column], result,  all_labels)\n",
    "    #get_top_discrimintary_keywords(result[NAIVE_BAYES][\"m\"],0)\n",
    "    summary_rec = {\"df\": df,\n",
    "                   \"df_report\" :df_report,\n",
    "                   \"model_result\" : result,\n",
    "                   \"meta\":[\n",
    "                       { \"name\": \"dataset\" , \"value\": meta_dataset},\n",
    "                       { \"name\": \"T\" , \"value\": meta_target}\n",
    "                          ]\n",
    "                  }\n",
    "\n",
    "\n",
    "    return  summary_rec\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GT Train predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Naive Bayes\n",
      "Max words :  202\n",
      "Training Naive Bayes + Tree\n",
      "Max words :  231\n",
      "Extracting features..for 549367\n",
      "Completed..\n",
      "0.0017259533996771825\n",
      "==================================================\n",
      "   Cohen's Kappa for 2 Raters (Weights: equal)    \n",
      "==================================================\n",
      "Subjects = 549367\n",
      "  Raters = 2\n",
      "   Kappa = 0.002\n",
      "\n",
      "       z = 1.664\n",
      " p-value = 0.096\n",
      "==================================================\n",
      "\n",
      "0.20238939328156325\n",
      "==================================================\n",
      "   Cohen's Kappa for 2 Raters (Weights: equal)    \n",
      "==================================================\n",
      "Subjects = 549367\n",
      "  Raters = 2\n",
      "   Kappa = 0.200\n",
      "\n",
      "       z = 192.449\n",
      " p-value = 0.000\n",
      "==================================================\n",
      "\n",
      "\\begin{tabular}{lrlrrrr}\n",
      "\\toprule\n",
      "             model & precision &         index & recall & f1-score & support &    cohen \\\\\n",
      "\\midrule\n",
      "       Naive Bayes &      33.4 & contradiction &   28.0 &     30.5 &  183187 &      NaN \\\\\n",
      "       Naive Bayes &      33.5 &    entailment &   45.4 &     38.5 &  183416 &      NaN \\\\\n",
      "       Naive Bayes &      33.4 &       neutral &   27.0 &     29.9 &  182764 &      NaN \\\\\n",
      "       Naive Bayes &      33.5 &      accuracy &   33.5 &     33.5 &       0 & 0.334583 \\\\\n",
      "       Naive Bayes &      33.5 &     macro avg &   33.4 &     33.0 &  549367 &      NaN \\\\\n",
      "       Naive Bayes &      33.5 &  weighted avg &   33.5 &     33.0 &  549367 &      NaN \\\\\n",
      "       Naive Bayes &       NaN &  python\\_kappa &    NaN &      NaN &  549367 & 0.001726 \\\\\n",
      "       Naive Bayes &       NaN &       r\\_kappa &    NaN &      NaN &  549367 & 0.001713 \\\\\n",
      "       Naive Bayes &       NaN &      r\\_pvalue &    NaN &      NaN &  549367 & 0.096107 \\\\\n",
      "Naive Bayes + Tree &      47.8 & contradiction &   40.0 &     43.6 &  183187 &      NaN \\\\\n",
      "Naive Bayes + Tree &      45.0 &    entailment &   56.6 &     50.1 &  183416 &      NaN \\\\\n",
      "Naive Bayes + Tree &      48.5 &       neutral &   43.9 &     46.1 &  182764 &      NaN \\\\\n",
      "Naive Bayes + Tree &      46.8 &      accuracy &   46.8 &     46.8 &       0 & 0.468308 \\\\\n",
      "Naive Bayes + Tree &      47.1 &     macro avg &   46.8 &     46.6 &  549367 &      NaN \\\\\n",
      "Naive Bayes + Tree &      47.1 &  weighted avg &   46.8 &     46.6 &  549367 &      NaN \\\\\n",
      "Naive Bayes + Tree &       NaN &  python\\_kappa &    NaN &      NaN &  549367 & 0.202389 \\\\\n",
      "Naive Bayes + Tree &       NaN &       r\\_kappa &    NaN &      NaN &  549367 & 0.200387 \\\\\n",
      "Naive Bayes + Tree &       NaN &      r\\_pvalue &    NaN &      NaN &  549367 & 0.000000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Training Naive Bayes\n",
      "Max words :  202\n",
      "Training Naive Bayes + Tree\n",
      "Max words :  231\n",
      "Extracting features..for 549367\n",
      "Completed..\n",
      "0.0017259533996771825\n",
      "==================================================\n",
      "   Cohen's Kappa for 2 Raters (Weights: equal)    \n",
      "==================================================\n",
      "Subjects = 549367\n",
      "  Raters = 2\n",
      "   Kappa = 0.002\n",
      "\n",
      "       z = 1.664\n",
      " p-value = 0.096\n",
      "==================================================\n",
      "\n",
      "0.20238939328156325\n",
      "==================================================\n",
      "   Cohen's Kappa for 2 Raters (Weights: equal)    \n",
      "==================================================\n",
      "Subjects = 549367\n",
      "  Raters = 2\n",
      "   Kappa = 0.200\n",
      "\n",
      "       z = 192.449\n",
      " p-value = 0.000\n",
      "==================================================\n",
      "\n",
      "\\begin{tabular}{lrlrrrr}\n",
      "\\toprule\n",
      "             model & precision &         index & recall & f1-score & support &    cohen \\\\\n",
      "\\midrule\n",
      "       Naive Bayes &      33.4 & contradiction &   28.0 &     30.5 &  183187 &      NaN \\\\\n",
      "       Naive Bayes &      33.5 &    entailment &   45.4 &     38.5 &  183416 &      NaN \\\\\n",
      "       Naive Bayes &      33.4 &       neutral &   27.0 &     29.9 &  182764 &      NaN \\\\\n",
      "       Naive Bayes &      33.5 &      accuracy &   33.5 &     33.5 &       0 & 0.334583 \\\\\n",
      "       Naive Bayes &      33.5 &     macro avg &   33.4 &     33.0 &  549367 &      NaN \\\\\n",
      "       Naive Bayes &      33.5 &  weighted avg &   33.5 &     33.0 &  549367 &      NaN \\\\\n",
      "       Naive Bayes &       NaN &  python\\_kappa &    NaN &      NaN &  549367 & 0.001726 \\\\\n",
      "       Naive Bayes &       NaN &       r\\_kappa &    NaN &      NaN &  549367 & 0.001713 \\\\\n",
      "       Naive Bayes &       NaN &      r\\_pvalue &    NaN &      NaN &  549367 & 0.096107 \\\\\n",
      "Naive Bayes + Tree &      47.8 & contradiction &   40.0 &     43.6 &  183187 &      NaN \\\\\n",
      "Naive Bayes + Tree &      45.0 &    entailment &   56.6 &     50.1 &  183416 &      NaN \\\\\n",
      "Naive Bayes + Tree &      48.5 &       neutral &   43.9 &     46.1 &  182764 &      NaN \\\\\n",
      "Naive Bayes + Tree &      46.8 &      accuracy &   46.8 &     46.8 &       0 & 0.468308 \\\\\n",
      "Naive Bayes + Tree &      47.1 &     macro avg &   46.8 &     46.6 &  549367 &      NaN \\\\\n",
      "Naive Bayes + Tree &      47.1 &  weighted avg &   46.8 &     46.6 &  549367 &      NaN \\\\\n",
      "Naive Bayes + Tree &       NaN &  python\\_kappa &    NaN &      NaN &  549367 & 0.202389 \\\\\n",
      "Naive Bayes + Tree &       NaN &       r\\_kappa &    NaN &      NaN &  549367 & 0.200387 \\\\\n",
      "Naive Bayes + Tree &       NaN &      r\\_pvalue &    NaN &      NaN &  549367 & 0.000000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Training Naive Bayes\n",
      "Max words :  202\n",
      "Training Naive Bayes + Tree\n",
      "Max words :  231\n",
      "Extracting features..for 549367\n",
      "Completed..\n",
      "0.0017259533996771825\n",
      "==================================================\n",
      "   Cohen's Kappa for 2 Raters (Weights: equal)    \n",
      "==================================================\n",
      "Subjects = 549367\n",
      "  Raters = 2\n",
      "   Kappa = 0.002\n",
      "\n",
      "       z = 1.664\n",
      " p-value = 0.096\n",
      "==================================================\n",
      "\n",
      "0.20238939328156325\n",
      "==================================================\n",
      "   Cohen's Kappa for 2 Raters (Weights: equal)    \n",
      "==================================================\n",
      "Subjects = 549367\n",
      "  Raters = 2\n",
      "   Kappa = 0.200\n",
      "\n",
      "       z = 192.449\n",
      " p-value = 0.000\n",
      "==================================================\n",
      "\n",
      "\\begin{tabular}{lrlrrrr}\n",
      "\\toprule\n",
      "             model & precision &         index & recall & f1-score & support &    cohen \\\\\n",
      "\\midrule\n",
      "       Naive Bayes &      33.4 & contradiction &   28.0 &     30.5 &  183187 &      NaN \\\\\n",
      "       Naive Bayes &      33.5 &    entailment &   45.4 &     38.5 &  183416 &      NaN \\\\\n",
      "       Naive Bayes &      33.4 &       neutral &   27.0 &     29.9 &  182764 &      NaN \\\\\n",
      "       Naive Bayes &      33.5 &      accuracy &   33.5 &     33.5 &       0 & 0.334583 \\\\\n",
      "       Naive Bayes &      33.5 &     macro avg &   33.4 &     33.0 &  549367 &      NaN \\\\\n",
      "       Naive Bayes &      33.5 &  weighted avg &   33.5 &     33.0 &  549367 &      NaN \\\\\n",
      "       Naive Bayes &       NaN &  python\\_kappa &    NaN &      NaN &  549367 & 0.001726 \\\\\n",
      "       Naive Bayes &       NaN &       r\\_kappa &    NaN &      NaN &  549367 & 0.001713 \\\\\n",
      "       Naive Bayes &       NaN &      r\\_pvalue &    NaN &      NaN &  549367 & 0.096107 \\\\\n",
      "Naive Bayes + Tree &      47.8 & contradiction &   40.0 &     43.6 &  183187 &      NaN \\\\\n",
      "Naive Bayes + Tree &      45.0 &    entailment &   56.6 &     50.1 &  183416 &      NaN \\\\\n",
      "Naive Bayes + Tree &      48.5 &       neutral &   43.9 &     46.1 &  182764 &      NaN \\\\\n",
      "Naive Bayes + Tree &      46.8 &      accuracy &   46.8 &     46.8 &       0 & 0.468308 \\\\\n",
      "Naive Bayes + Tree &      47.1 &     macro avg &   46.8 &     46.6 &  549367 &      NaN \\\\\n",
      "Naive Bayes + Tree &      47.1 &  weighted avg &   46.8 &     46.6 &  549367 &      NaN \\\\\n",
      "Naive Bayes + Tree &       NaN &  python\\_kappa &    NaN &      NaN &  549367 & 0.202389 \\\\\n",
      "Naive Bayes + Tree &       NaN &       r\\_kappa &    NaN &      NaN &  549367 & 0.200387 \\\\\n",
      "Naive Bayes + Tree &       NaN &      r\\_pvalue &    NaN &      NaN &  549367 & 0.000000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = load_jsonl(train_file).pipe(rename_columns)\\\n",
    "                                .pipe(filter_empty_label)\\\n",
    "                                .pipe(add_join_hyp_premise)\n",
    "\n",
    "run_analysis(df_train, x_column=\"premise\", y_column=\"label\", meta_dataset=\"SNLI TR PRM\", meta_target=\"GT\")\n",
    "run_analysis(df_train, x_column=\"hypothesis\", y_column=\"label\", meta_dataset=\"SNLI TR HYP\", meta_target=\"GT\")\n",
    "\n",
    "r = run_analysis(df_train, x_column=\"prem_hyp\", y_column=\"label\", meta_dataset=\"SNLI TR\", meta_target=\"GT\")\n",
    "all_results.append(r)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Test on Test GT fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Naive Bayes\n",
      "Max words :  215\n",
      "Training Naive Bayes + Tree\n",
      "Max words :  244\n",
      "Extracting features..for 9824\n",
      "Completed..\n",
      "0.03686705214146879\n",
      "==================================================\n",
      "   Cohen's Kappa for 2 Raters (Weights: equal)    \n",
      "==================================================\n",
      "Subjects = 9824\n",
      "  Raters = 2\n",
      "   Kappa = 0.039\n",
      "\n",
      "       z = 5.137\n",
      " p-value = 0.000\n",
      "==================================================\n",
      "\n",
      "0.2314070876593587\n",
      "==================================================\n",
      "   Cohen's Kappa for 2 Raters (Weights: equal)    \n",
      "==================================================\n",
      "Subjects = 9824\n",
      "  Raters = 2\n",
      "   Kappa = 0.230\n",
      "\n",
      "       z = 29.449\n",
      " p-value = 0.000\n",
      "==================================================\n",
      "\n",
      "\\begin{tabular}{lrlrrrr}\n",
      "\\toprule\n",
      "             model & precision &         index & recall & f1-score & support &        cohen \\\\\n",
      "\\midrule\n",
      "       Naive Bayes &      35.6 & contradiction &   22.2 &     27.4 &    3237 &          NaN \\\\\n",
      "       Naive Bayes &      36.1 &    entailment &   48.3 &     41.3 &    3368 &          NaN \\\\\n",
      "       Naive Bayes &      36.0 &       neutral &   36.8 &     36.4 &    3219 &          NaN \\\\\n",
      "       Naive Bayes &      36.0 &      accuracy &   36.0 &     36.0 &       0 & 3.595277e-01 \\\\\n",
      "       Naive Bayes &      35.9 &     macro avg &   35.8 &     35.0 &    9824 &          NaN \\\\\n",
      "       Naive Bayes &      35.9 &  weighted avg &   36.0 &     35.1 &    9824 &          NaN \\\\\n",
      "       Naive Bayes &       NaN &  python\\_kappa &    NaN &      NaN &    9824 & 3.686705e-02 \\\\\n",
      "       Naive Bayes &       NaN &       r\\_kappa &    NaN &      NaN &    9824 & 3.890661e-02 \\\\\n",
      "       Naive Bayes &       NaN &      r\\_pvalue &    NaN &      NaN &    9824 & 2.790869e-07 \\\\\n",
      "Naive Bayes + Tree &      48.8 & contradiction &   41.6 &     44.9 &    3237 &          NaN \\\\\n",
      "Naive Bayes + Tree &      47.8 &    entailment &   56.8 &     51.9 &    3368 &          NaN \\\\\n",
      "Naive Bayes + Tree &      50.2 &       neutral &   47.7 &     48.9 &    3219 &          NaN \\\\\n",
      "Naive Bayes + Tree &      48.8 &      accuracy &   48.8 &     48.8 &       0 & 4.883958e-01 \\\\\n",
      "Naive Bayes + Tree &      48.9 &     macro avg &   48.7 &     48.6 &    9824 &          NaN \\\\\n",
      "Naive Bayes + Tree &      48.9 &  weighted avg &   48.8 &     48.6 &    9824 &          NaN \\\\\n",
      "Naive Bayes + Tree &       NaN &  python\\_kappa &    NaN &      NaN &    9824 & 2.314071e-01 \\\\\n",
      "Naive Bayes + Tree &       NaN &       r\\_kappa &    NaN &      NaN &    9824 & 2.299022e-01 \\\\\n",
      "Naive Bayes + Tree &       NaN &      r\\_pvalue &    NaN &      NaN &    9824 & 0.000000e+00 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Training Naive Bayes\n",
      "Max words :  215\n",
      "Training Naive Bayes + Tree\n",
      "Max words :  244\n",
      "Extracting features..for 9824\n",
      "Completed..\n",
      "0.03686705214146879\n",
      "==================================================\n",
      "   Cohen's Kappa for 2 Raters (Weights: equal)    \n",
      "==================================================\n",
      "Subjects = 9824\n",
      "  Raters = 2\n",
      "   Kappa = 0.039\n",
      "\n",
      "       z = 5.137\n",
      " p-value = 0.000\n",
      "==================================================\n",
      "\n",
      "0.2314070876593587\n",
      "==================================================\n",
      "   Cohen's Kappa for 2 Raters (Weights: equal)    \n",
      "==================================================\n",
      "Subjects = 9824\n",
      "  Raters = 2\n",
      "   Kappa = 0.230\n",
      "\n",
      "       z = 29.449\n",
      " p-value = 0.000\n",
      "==================================================\n",
      "\n",
      "\\begin{tabular}{lrlrrrr}\n",
      "\\toprule\n",
      "             model & precision &         index & recall & f1-score & support &        cohen \\\\\n",
      "\\midrule\n",
      "       Naive Bayes &      35.6 & contradiction &   22.2 &     27.4 &    3237 &          NaN \\\\\n",
      "       Naive Bayes &      36.1 &    entailment &   48.3 &     41.3 &    3368 &          NaN \\\\\n",
      "       Naive Bayes &      36.0 &       neutral &   36.8 &     36.4 &    3219 &          NaN \\\\\n",
      "       Naive Bayes &      36.0 &      accuracy &   36.0 &     36.0 &       0 & 3.595277e-01 \\\\\n",
      "       Naive Bayes &      35.9 &     macro avg &   35.8 &     35.0 &    9824 &          NaN \\\\\n",
      "       Naive Bayes &      35.9 &  weighted avg &   36.0 &     35.1 &    9824 &          NaN \\\\\n",
      "       Naive Bayes &       NaN &  python\\_kappa &    NaN &      NaN &    9824 & 3.686705e-02 \\\\\n",
      "       Naive Bayes &       NaN &       r\\_kappa &    NaN &      NaN &    9824 & 3.890661e-02 \\\\\n",
      "       Naive Bayes &       NaN &      r\\_pvalue &    NaN &      NaN &    9824 & 2.790869e-07 \\\\\n",
      "Naive Bayes + Tree &      48.8 & contradiction &   41.6 &     44.9 &    3237 &          NaN \\\\\n",
      "Naive Bayes + Tree &      47.8 &    entailment &   56.8 &     51.9 &    3368 &          NaN \\\\\n",
      "Naive Bayes + Tree &      50.2 &       neutral &   47.7 &     48.9 &    3219 &          NaN \\\\\n",
      "Naive Bayes + Tree &      48.8 &      accuracy &   48.8 &     48.8 &       0 & 4.883958e-01 \\\\\n",
      "Naive Bayes + Tree &      48.9 &     macro avg &   48.7 &     48.6 &    9824 &          NaN \\\\\n",
      "Naive Bayes + Tree &      48.9 &  weighted avg &   48.8 &     48.6 &    9824 &          NaN \\\\\n",
      "Naive Bayes + Tree &       NaN &  python\\_kappa &    NaN &      NaN &    9824 & 2.314071e-01 \\\\\n",
      "Naive Bayes + Tree &       NaN &       r\\_kappa &    NaN &      NaN &    9824 & 2.299022e-01 \\\\\n",
      "Naive Bayes + Tree &       NaN &      r\\_pvalue &    NaN &      NaN &    9824 & 0.000000e+00 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Training Naive Bayes\n",
      "Max words :  215\n",
      "Training Naive Bayes + Tree\n",
      "Max words :  244\n",
      "Extracting features..for 9824\n",
      "Completed..\n",
      "0.03686705214146879\n",
      "==================================================\n",
      "   Cohen's Kappa for 2 Raters (Weights: equal)    \n",
      "==================================================\n",
      "Subjects = 9824\n",
      "  Raters = 2\n",
      "   Kappa = 0.039\n",
      "\n",
      "       z = 5.137\n",
      " p-value = 0.000\n",
      "==================================================\n",
      "\n",
      "0.2314070876593587\n",
      "==================================================\n",
      "   Cohen's Kappa for 2 Raters (Weights: equal)    \n",
      "==================================================\n",
      "Subjects = 9824\n",
      "  Raters = 2\n",
      "   Kappa = 0.230\n",
      "\n",
      "       z = 29.449\n",
      " p-value = 0.000\n",
      "==================================================\n",
      "\n",
      "\\begin{tabular}{lrlrrrr}\n",
      "\\toprule\n",
      "             model & precision &         index & recall & f1-score & support &        cohen \\\\\n",
      "\\midrule\n",
      "       Naive Bayes &      35.6 & contradiction &   22.2 &     27.4 &    3237 &          NaN \\\\\n",
      "       Naive Bayes &      36.1 &    entailment &   48.3 &     41.3 &    3368 &          NaN \\\\\n",
      "       Naive Bayes &      36.0 &       neutral &   36.8 &     36.4 &    3219 &          NaN \\\\\n",
      "       Naive Bayes &      36.0 &      accuracy &   36.0 &     36.0 &       0 & 3.595277e-01 \\\\\n",
      "       Naive Bayes &      35.9 &     macro avg &   35.8 &     35.0 &    9824 &          NaN \\\\\n",
      "       Naive Bayes &      35.9 &  weighted avg &   36.0 &     35.1 &    9824 &          NaN \\\\\n",
      "       Naive Bayes &       NaN &  python\\_kappa &    NaN &      NaN &    9824 & 3.686705e-02 \\\\\n",
      "       Naive Bayes &       NaN &       r\\_kappa &    NaN &      NaN &    9824 & 3.890661e-02 \\\\\n",
      "       Naive Bayes &       NaN &      r\\_pvalue &    NaN &      NaN &    9824 & 2.790869e-07 \\\\\n",
      "Naive Bayes + Tree &      48.8 & contradiction &   41.6 &     44.9 &    3237 &          NaN \\\\\n",
      "Naive Bayes + Tree &      47.8 &    entailment &   56.8 &     51.9 &    3368 &          NaN \\\\\n",
      "Naive Bayes + Tree &      50.2 &       neutral &   47.7 &     48.9 &    3219 &          NaN \\\\\n",
      "Naive Bayes + Tree &      48.8 &      accuracy &   48.8 &     48.8 &       0 & 4.883958e-01 \\\\\n",
      "Naive Bayes + Tree &      48.9 &     macro avg &   48.7 &     48.6 &    9824 &          NaN \\\\\n",
      "Naive Bayes + Tree &      48.9 &  weighted avg &   48.8 &     48.6 &    9824 &          NaN \\\\\n",
      "Naive Bayes + Tree &       NaN &  python\\_kappa &    NaN &      NaN &    9824 & 2.314071e-01 \\\\\n",
      "Naive Bayes + Tree &       NaN &       r\\_kappa &    NaN &      NaN &    9824 & 2.299022e-01 \\\\\n",
      "Naive Bayes + Tree &       NaN &      r\\_pvalue &    NaN &      NaN &    9824 & 0.000000e+00 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = load_json(test_predictions_file).pipe(add_join_hyp_premise)\n",
    "\n",
    "run_analysis(df_test, x_column=\"premise\", y_column=\"label\", meta_dataset=\"SNLI TS PRM\", meta_target=\"GT\")\n",
    "run_analysis(df_test, x_column=\"hypothesis\", y_column=\"label\", meta_dataset=\"SNLI TS HYP\", meta_target=\"GT\")\n",
    "\n",
    "r = run_analysis(df_test, x_column=\"prem_hyp\", y_column=\"label\", meta_dataset=\"SNLI TS\", meta_target=\"GT\")\n",
    "all_results.append(r)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Naive Bayes\n",
      "Max words :  215\n",
      "Training Naive Bayes + Tree\n",
      "Max words :  244\n",
      "Extracting features..for 9824\n",
      "Completed..\n",
      "0.016707298682237526\n",
      "==================================================\n",
      "   Cohen's Kappa for 2 Raters (Weights: equal)    \n",
      "==================================================\n",
      "Subjects = 9824\n",
      "  Raters = 2\n",
      "   Kappa = 0.017\n",
      "\n",
      "       z = 2.201\n",
      " p-value = 0.028\n",
      "==================================================\n",
      "\n",
      "0.22319327774990005\n",
      "==================================================\n",
      "   Cohen's Kappa for 2 Raters (Weights: equal)    \n",
      "==================================================\n",
      "Subjects = 9824\n",
      "  Raters = 2\n",
      "   Kappa = 0.219\n",
      "\n",
      "       z = 28.020\n",
      " p-value = 0.000\n",
      "==================================================\n",
      "\n",
      "\\begin{tabular}{lrlrrrr}\n",
      "\\toprule\n",
      "             model & precision &         index & recall & f1-score & support &    cohen \\\\\n",
      "\\midrule\n",
      "       Naive Bayes &      33.4 & contradiction &   21.1 &     25.9 &    3192 &      NaN \\\\\n",
      "       Naive Bayes &      34.2 &    entailment &   47.2 &     39.6 &    3270 &      NaN \\\\\n",
      "       Naive Bayes &      35.8 &       neutral &   35.0 &     35.4 &    3362 &      NaN \\\\\n",
      "       Naive Bayes &      34.5 &      accuracy &   34.5 &     34.5 &       0 & 0.345480 \\\\\n",
      "       Naive Bayes &      34.4 &     macro avg &   34.4 &     33.6 &    9824 &      NaN \\\\\n",
      "       Naive Bayes &      34.5 &  weighted avg &   34.5 &     33.7 &    9824 &      NaN \\\\\n",
      "       Naive Bayes &       NaN &  python\\_kappa &    NaN &      NaN &    9824 & 0.016707 \\\\\n",
      "       Naive Bayes &       NaN &       r\\_kappa &    NaN &      NaN &    9824 & 0.016733 \\\\\n",
      "       Naive Bayes &       NaN &      r\\_pvalue &    NaN &      NaN &    9824 & 0.027724 \\\\\n",
      "Naive Bayes + Tree &      47.5 & contradiction &   41.2 &     44.1 &    3192 &      NaN \\\\\n",
      "Naive Bayes + Tree &      46.6 &    entailment &   57.0 &     51.3 &    3270 &      NaN \\\\\n",
      "Naive Bayes + Tree &      51.0 &       neutral &   46.4 &     48.6 &    3362 &      NaN \\\\\n",
      "Naive Bayes + Tree &      48.2 &      accuracy &   48.2 &     48.2 &       0 & 0.482288 \\\\\n",
      "Naive Bayes + Tree &      48.4 &     macro avg &   48.2 &     48.0 &    9824 &      NaN \\\\\n",
      "Naive Bayes + Tree &      48.4 &  weighted avg &   48.2 &     48.0 &    9824 &      NaN \\\\\n",
      "Naive Bayes + Tree &       NaN &  python\\_kappa &    NaN &      NaN &    9824 & 0.223193 \\\\\n",
      "Naive Bayes + Tree &       NaN &       r\\_kappa &    NaN &      NaN &    9824 & 0.219113 \\\\\n",
      "Naive Bayes + Tree &       NaN &      r\\_pvalue &    NaN &      NaN &    9824 & 0.000000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Training Naive Bayes\n",
      "Max words :  215\n",
      "Training Naive Bayes + Tree\n",
      "Max words :  244\n",
      "Extracting features..for 9824\n",
      "Completed..\n",
      "0.016707298682237526\n",
      "==================================================\n",
      "   Cohen's Kappa for 2 Raters (Weights: equal)    \n",
      "==================================================\n",
      "Subjects = 9824\n",
      "  Raters = 2\n",
      "   Kappa = 0.017\n",
      "\n",
      "       z = 2.201\n",
      " p-value = 0.028\n",
      "==================================================\n",
      "\n",
      "0.22319327774990005\n",
      "==================================================\n",
      "   Cohen's Kappa for 2 Raters (Weights: equal)    \n",
      "==================================================\n",
      "Subjects = 9824\n",
      "  Raters = 2\n",
      "   Kappa = 0.219\n",
      "\n",
      "       z = 28.020\n",
      " p-value = 0.000\n",
      "==================================================\n",
      "\n",
      "\\begin{tabular}{lrlrrrr}\n",
      "\\toprule\n",
      "             model & precision &         index & recall & f1-score & support &    cohen \\\\\n",
      "\\midrule\n",
      "       Naive Bayes &      33.4 & contradiction &   21.1 &     25.9 &    3192 &      NaN \\\\\n",
      "       Naive Bayes &      34.2 &    entailment &   47.2 &     39.6 &    3270 &      NaN \\\\\n",
      "       Naive Bayes &      35.8 &       neutral &   35.0 &     35.4 &    3362 &      NaN \\\\\n",
      "       Naive Bayes &      34.5 &      accuracy &   34.5 &     34.5 &       0 & 0.345480 \\\\\n",
      "       Naive Bayes &      34.4 &     macro avg &   34.4 &     33.6 &    9824 &      NaN \\\\\n",
      "       Naive Bayes &      34.5 &  weighted avg &   34.5 &     33.7 &    9824 &      NaN \\\\\n",
      "       Naive Bayes &       NaN &  python\\_kappa &    NaN &      NaN &    9824 & 0.016707 \\\\\n",
      "       Naive Bayes &       NaN &       r\\_kappa &    NaN &      NaN &    9824 & 0.016733 \\\\\n",
      "       Naive Bayes &       NaN &      r\\_pvalue &    NaN &      NaN &    9824 & 0.027724 \\\\\n",
      "Naive Bayes + Tree &      47.5 & contradiction &   41.2 &     44.1 &    3192 &      NaN \\\\\n",
      "Naive Bayes + Tree &      46.6 &    entailment &   57.0 &     51.3 &    3270 &      NaN \\\\\n",
      "Naive Bayes + Tree &      51.0 &       neutral &   46.4 &     48.6 &    3362 &      NaN \\\\\n",
      "Naive Bayes + Tree &      48.2 &      accuracy &   48.2 &     48.2 &       0 & 0.482288 \\\\\n",
      "Naive Bayes + Tree &      48.4 &     macro avg &   48.2 &     48.0 &    9824 &      NaN \\\\\n",
      "Naive Bayes + Tree &      48.4 &  weighted avg &   48.2 &     48.0 &    9824 &      NaN \\\\\n",
      "Naive Bayes + Tree &       NaN &  python\\_kappa &    NaN &      NaN &    9824 & 0.223193 \\\\\n",
      "Naive Bayes + Tree &       NaN &       r\\_kappa &    NaN &      NaN &    9824 & 0.219113 \\\\\n",
      "Naive Bayes + Tree &       NaN &      r\\_pvalue &    NaN &      NaN &    9824 & 0.000000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "Training Naive Bayes\n",
      "Max words :  215\n",
      "Training Naive Bayes + Tree\n",
      "Max words :  244\n",
      "Extracting features..for 9824\n",
      "Completed..\n",
      "0.016707298682237526\n",
      "==================================================\n",
      "   Cohen's Kappa for 2 Raters (Weights: equal)    \n",
      "==================================================\n",
      "Subjects = 9824\n",
      "  Raters = 2\n",
      "   Kappa = 0.017\n",
      "\n",
      "       z = 2.201\n",
      " p-value = 0.028\n",
      "==================================================\n",
      "\n",
      "0.22319327774990005\n",
      "==================================================\n",
      "   Cohen's Kappa for 2 Raters (Weights: equal)    \n",
      "==================================================\n",
      "Subjects = 9824\n",
      "  Raters = 2\n",
      "   Kappa = 0.219\n",
      "\n",
      "       z = 28.020\n",
      " p-value = 0.000\n",
      "==================================================\n",
      "\n",
      "\\begin{tabular}{lrlrrrr}\n",
      "\\toprule\n",
      "             model & precision &         index & recall & f1-score & support &    cohen \\\\\n",
      "\\midrule\n",
      "       Naive Bayes &      33.4 & contradiction &   21.1 &     25.9 &    3192 &      NaN \\\\\n",
      "       Naive Bayes &      34.2 &    entailment &   47.2 &     39.6 &    3270 &      NaN \\\\\n",
      "       Naive Bayes &      35.8 &       neutral &   35.0 &     35.4 &    3362 &      NaN \\\\\n",
      "       Naive Bayes &      34.5 &      accuracy &   34.5 &     34.5 &       0 & 0.345480 \\\\\n",
      "       Naive Bayes &      34.4 &     macro avg &   34.4 &     33.6 &    9824 &      NaN \\\\\n",
      "       Naive Bayes &      34.5 &  weighted avg &   34.5 &     33.7 &    9824 &      NaN \\\\\n",
      "       Naive Bayes &       NaN &  python\\_kappa &    NaN &      NaN &    9824 & 0.016707 \\\\\n",
      "       Naive Bayes &       NaN &       r\\_kappa &    NaN &      NaN &    9824 & 0.016733 \\\\\n",
      "       Naive Bayes &       NaN &      r\\_pvalue &    NaN &      NaN &    9824 & 0.027724 \\\\\n",
      "Naive Bayes + Tree &      47.5 & contradiction &   41.2 &     44.1 &    3192 &      NaN \\\\\n",
      "Naive Bayes + Tree &      46.6 &    entailment &   57.0 &     51.3 &    3270 &      NaN \\\\\n",
      "Naive Bayes + Tree &      51.0 &       neutral &   46.4 &     48.6 &    3362 &      NaN \\\\\n",
      "Naive Bayes + Tree &      48.2 &      accuracy &   48.2 &     48.2 &       0 & 0.482288 \\\\\n",
      "Naive Bayes + Tree &      48.4 &     macro avg &   48.2 &     48.0 &    9824 &      NaN \\\\\n",
      "Naive Bayes + Tree &      48.4 &  weighted avg &   48.2 &     48.0 &    9824 &      NaN \\\\\n",
      "Naive Bayes + Tree &       NaN &  python\\_kappa &    NaN &      NaN &    9824 & 0.223193 \\\\\n",
      "Naive Bayes + Tree &       NaN &       r\\_kappa &    NaN &      NaN &    9824 & 0.219113 \\\\\n",
      "Naive Bayes + Tree &       NaN &      r\\_pvalue &    NaN &      NaN &    9824 & 0.000000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_analysis(df_test, x_column=\"premise\", y_column=\"predicted_label\", meta_dataset=\"SNLI TS PRM\", meta_target=\"BP\")\n",
    "run_analysis(df_test, x_column=\"hypothesis\", y_column=\"predicted_label\", meta_dataset=\"SNLI TS HYP\", meta_target=\"BP\")\n",
    "\n",
    "r=run_analysis(df_test, x_column=\"prem_hyp\", y_column=\"predicted_label\", meta_dataset=\"SNLI TS\", meta_target=\"BP\")\n",
    "all_results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_tree(result_test_bp[NAIVE_BAYES_WITH_TREE][\"m\"])\n",
    "# plot_tree_pretty(r_test_predicted_label[NAIVE_BAYES_WITH_TREE][\"m\"], \n",
    "#                  df_test.pipe(add_x_json_column)[\"x_json\"], \n",
    "#                  df_test[\"predicted_label\"], \"Test prediction fit\", \"test_prediction_treeplot.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latex results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllrlrrrr}\n",
      "\\toprule\n",
      "dataset &  T & model & precision &        index & recall & f1-score & support & cohen \\\\\n",
      "\\midrule\n",
      "SNLI TR & GT &    NB &       NaN & python\\_kappa &    NaN &      NaN &  549367 &  0.00 \\\\\n",
      "SNLI TR & GT &    NB &       NaN &      r\\_kappa &    NaN &      NaN &  549367 &  0.00 \\\\\n",
      "SNLI TR & GT &    NB &       NaN &     r\\_pvalue &    NaN &      NaN &  549367 &  0.10 \\\\\n",
      "SNLI TR & GT &   NBT &       NaN & python\\_kappa &    NaN &      NaN &  549367 &  0.20 \\\\\n",
      "SNLI TR & GT &   NBT &       NaN &      r\\_kappa &    NaN &      NaN &  549367 &  0.20 \\\\\n",
      "SNLI TR & GT &   NBT &       NaN &     r\\_pvalue &    NaN &      NaN &  549367 &  0.00 \\\\\n",
      "SNLI TS & GT &    NB &       NaN & python\\_kappa &    NaN &      NaN &    9824 &  0.04 \\\\\n",
      "SNLI TS & GT &    NB &       NaN &      r\\_kappa &    NaN &      NaN &    9824 &  0.04 \\\\\n",
      "SNLI TS & GT &    NB &       NaN &     r\\_pvalue &    NaN &      NaN &    9824 &  0.00 \\\\\n",
      "SNLI TS & GT &   NBT &       NaN & python\\_kappa &    NaN &      NaN &    9824 &  0.23 \\\\\n",
      "SNLI TS & GT &   NBT &       NaN &      r\\_kappa &    NaN &      NaN &    9824 &  0.23 \\\\\n",
      "SNLI TS & GT &   NBT &       NaN &     r\\_pvalue &    NaN &      NaN &    9824 &  0.00 \\\\\n",
      "SNLI TS & BP &    NB &       NaN & python\\_kappa &    NaN &      NaN &    9824 &  0.02 \\\\\n",
      "SNLI TS & BP &    NB &       NaN &      r\\_kappa &    NaN &      NaN &    9824 &  0.02 \\\\\n",
      "SNLI TS & BP &    NB &       NaN &     r\\_pvalue &    NaN &      NaN &    9824 &  0.03 \\\\\n",
      "SNLI TS & BP &   NBT &       NaN & python\\_kappa &    NaN &      NaN &    9824 &  0.22 \\\\\n",
      "SNLI TS & BP &   NBT &       NaN &      r\\_kappa &    NaN &      NaN &    9824 &  0.22 \\\\\n",
      "SNLI TS & BP &   NBT &       NaN &     r\\_pvalue &    NaN &      NaN &    9824 &  0.00 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_latex(meta_dict_df, index_filter=None ):#[\"micro avg\"]\n",
    "    result = []\n",
    "    model_names = {NAIVE_BAYES: \"NB\",\n",
    "                  NAIVE_BAYES_WITH_TREE:\"NBT\"\n",
    "    }\n",
    "\n",
    "    for item in meta_dict_df:\n",
    "        df= item[\"df_report\"].copy(deep=True)\n",
    "        if index_filter:\n",
    "            df = df[df[\"index\"].isin(index_filter)].copy(deep=True)\n",
    "        df[\"model\"] = df[\"model\"].apply(lambda x: model_names[x])\n",
    "        for i, meta in enumerate(item[\"meta\"]):\n",
    "            df.insert(i, meta[\"name\"], meta[\"value\"])\n",
    "        result.append(df)\n",
    "    df_summ = pd.concat(result)\n",
    "    \n",
    "    # Drop index if just one ..\n",
    "    if df[\"index\"].nunique() ==1:\n",
    "        df_summ = df_summ.drop(\"index\", axis=1)\n",
    "    \n",
    "    return df_summ\n",
    "    \n",
    "\n",
    "\n",
    "df_summ = prepare_latex (all_results, index_filter = [\"python_kappa\", \"r_kappa\", \"r_pvalue\"] )\n",
    "\n",
    "print(df_summ.to_latex(index=False, formatters = {\"precision\": float_percent_format,\n",
    "                                                  \"recall\":float_percent_format, \n",
    "                                                  \"f1-score\":float_percent_format,\n",
    "                                                  \"support\":int_format,\n",
    "                                                 \"cohen\":float_format}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
